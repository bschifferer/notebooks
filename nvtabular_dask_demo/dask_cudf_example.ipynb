{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU NVTabular: An Introduction to NVTabular + dask_cudf\n",
    "\n",
    "**Last Updated**: August 5th 2020\n",
    "\n",
    "## Overview of dask_cudf Integration in NVTabular 0.2.0\n",
    "\n",
    "As of the 0.2.0 release (nvtabular>=0.2.0), many components of NVTabular have been refactored to use [Dask](https://dask.org/).  More specifically, the following components are now based on the [RAPIDS](https://rapids.ai/) `dask_cudf` library:\n",
    "\n",
    "- **`nvtabular.Dataset`**: Most NVTabular functionality requires the raw data to be represented as a `Dataset` object. A `Dataset` can be initialized using file/directory paths (\"csv\" or \"parquet\"), a `pyarrow.Table`, a pandas/cudf `DataFrame`, or a pandas/cudf-based *Dask* `DataFrame`.  The purpose of this \"wrapper\" class is to provide other NVTabular components with reliable mechanisms to (1) translate the target data into a `dask_cudf.DataFrame`, and to (2) iterate over the target data in small-enough chunks to fit in GPU memory.\n",
    "- **`nvtabular.Workflow`**: The central class used in NVTabular to compose a GPU-accelerated preprocessing pipeline.  The Workflow class now keeps tracks the state of the underlying data by applying all operations to an internal `dask_cudf.DataFrame` object (`ddf`). If the user specifically chooses to iterate over the data to apply transform operations, the iteration will be over partitions of the internal `ddf`.\n",
    "- **`nvtabular.ops.StatOperator`**: All \"statistics-gathering\" operations must be designed to operate directly on the `Workflow` object's internal `ddf`.  This requirement facilitates the ability of NVTabular to handle the calculation of global statistics in a scalable way.\n",
    "\n",
    "**Big Picture**:  NVTabular is tightly integrated with `dask_cudf`.  By representing the underlying dataset as a (lazily-evaluated) collection of cudf DataFrame objects (i.e. a single `dask_cudf.DataFrame`), we can seamlessly scale our preprocessing workflow to multiple GPUs.\n",
    "\n",
    "## Simple Multi-GPU Toy Example\n",
    "In order to illustrate the `dask_cudf`-based functionality of NVTabular, we will walk through a simple preprocessing example using *toy* data.\n",
    "\n",
    "\n",
    "#### Step 1:  Import Libraries and Cleanup Working Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# External Dependencies\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a \"fast\" root directory for this example\n",
    "basedir = \"/raid/dask-space/rzamora\"\n",
    "\n",
    "# Define and clean our worker/output directories\n",
    "dask_workdir = os.path.join(basedir, \"workdir\")\n",
    "demo_output_path = os.path.join(basedir, \"demo_output\")\n",
    "demo_dataset_path = os.path.join(basedir, \"demo_dataset.parquet\")\n",
    "\n",
    "# Make sure we have a clean worker space for Dask\n",
    "if os.path.isdir(dask_workdir):\n",
    "    shutil.rmtree(dask_workdir)\n",
    "    os.mkdir(dask_workdir)\n",
    "\n",
    "# Make sure we have a clean output path\n",
    "if os.path.isdir(demo_output_path):\n",
    "    shutil.rmtree(demo_output_path)\n",
    "    os.mkdir(demo_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create a \"Toy\" Parquet Dataset\n",
    "In order to illustrate the power multi-GPU scaling, without requiring an excessive runtime, we can use the `cudf.datasets.timeseries` API to generate a 20GB toy dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(demo_dataset_path):\n",
    "    # Write a \"largish\" dataset (~20GB)\n",
    "    nwrites = 25\n",
    "    pw = cudf.io.parquet.ParquetWriter(demo_dataset_path)\n",
    "    for i in range(nwrites):\n",
    "        df = cudf.datasets.timeseries(start='2000-01-01', end='2000-12-31', freq='1s', seed=i).reset_index(drop=False)\n",
    "        df[\"name\"] = df[\"name\"].astype(\"object\")\n",
    "        df[\"label\"] = cp.random.choice(cp.array([0, 1], dtype=\"uint8\"), len(df))\n",
    "        pw.write_table(df)\n",
    "    pw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create an NVTabular `Dataset` object\n",
    "\n",
    "As discussed above, the `nvt.Workflow` class requires data to be represented as an `nvt.Dataset`. This convention allows NVTabular to abstract way the raw format of the data, and convert everything to a consistent `dask_cudf.DataFrame` representation. Since the `Dataset` API effectively wraps functions like `dask_cudf.read_csv`, the syntax is very simple, and the execution time is insignificant.\n",
    "\n",
    "**Important `Dataset` Considerations**:\n",
    "\n",
    "- Can be initialized with the following objects:\n",
    "    - 1+ file/directory paths. An `engine` argument is required to specify the file format (unless file names are appended with `csv` or `parquet`)\n",
    "    - `cudf.DataFrame`. Internal `ddf` will have 1 partition.\n",
    "    - `pandas.DataFrame`. Internal `ddf` will have 1 partition.\n",
    "    - `pyarrow.Table`. Internal `ddf` will have 1 partition.\n",
    "    - `dask_cudf.DataFrame`. Internal `ddf` will be a shallow copy of the input.\n",
    "    - `dask.dataframe.DataFrame`. Internal `ddf` will be a direct pandas->cudf conversion of the input.\n",
    "- For file-based data initialization, the size of the internall `ddf` partitions will be chosen according to the following arguments (in order of precedence):\n",
    "    - `part_size`: Desired maximum size of each partition **in bytes**.  Note that you can pass a string here. like `\"2GB\"`.\n",
    "    - `part_mem_fraction`: Desired maximum size of each partition as a **fraction of total GPU memory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 774 ms, sys: 519 ms, total: 1.29 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "# Create a Dataset\n",
    "# (`engine` argument optional if file names appended with `csv` or `parquet`)\n",
    "%time ds = nvt.Dataset(demo_dataset_path, engine=\"parquet\", part_size=\"1.2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your data is converted to a `Dataset` object, it can be converted to a `dask_cudf.DataFrame` using the `to_ddf` method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>1019</td>\n",
       "      <td>Michael</td>\n",
       "      <td>0.168205</td>\n",
       "      <td>-0.547230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01 00:00:01</td>\n",
       "      <td>984</td>\n",
       "      <td>Patricia</td>\n",
       "      <td>-0.145077</td>\n",
       "      <td>-0.240521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01 00:00:02</td>\n",
       "      <td>935</td>\n",
       "      <td>Victor</td>\n",
       "      <td>0.557024</td>\n",
       "      <td>-0.098855</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01 00:00:03</td>\n",
       "      <td>970</td>\n",
       "      <td>Alice</td>\n",
       "      <td>0.527366</td>\n",
       "      <td>-0.632569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01 00:00:04</td>\n",
       "      <td>997</td>\n",
       "      <td>Dan</td>\n",
       "      <td>0.309193</td>\n",
       "      <td>0.704845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp    id      name         x         y  label\n",
       "0 2000-01-01 00:00:00  1019   Michael  0.168205 -0.547230      1\n",
       "1 2000-01-01 00:00:01   984  Patricia -0.145077 -0.240521      0\n",
       "2 2000-01-01 00:00:02   935    Victor  0.557024 -0.098855      1\n",
       "3 2000-01-01 00:00:03   970     Alice  0.527366 -0.632569      1\n",
       "4 2000-01-01 00:00:04   997       Dan  0.309193  0.704845      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_ddf().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of a Dataset (a `ddf`) can be used to initialize a new Dataset.  This means we can use `dask_cudf` to perform complex ETL on our data before we process it in a `Workflow`. For example, although NVTabular does not support global shuffling transformations (yet), these operations **can** be performed before (or after) a Workflow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=29</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[us]</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>uint8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: simple-shuffle-combine, 928 tasks</div>"
      ],
      "text/plain": [
       "<dask_cudf.DataFrame | 928 tasks | 29 npartitions>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = ds.to_ddf().shuffle(\"id\", ignore_index=True)\n",
    "ds = nvt.Dataset(ddf)\n",
    "ds.to_ddf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds\n",
    "del ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Distributed Cluster Deployment\n",
    "\n",
    "Before we walk through the rest of this multi-GPU preprocessing example, it is important to reiterate that `dask_cudf` is used extensively within NVTabular.  This essentially means that you do **not** need to do anything special to *use* Dask here.  With that said, the default behavior of NVTabular is to to utilize Dask's [\"synchronous\"](https://docs.dask.org/en/latest/scheduling.html) task scheduler, which precludes distributed processing.  In order to properly utilize a multi-GPU system, you need to deploy a `dask.distributed` *cluster*.\n",
    "\n",
    "There are many different ways to create a distributed Dask cluster.  In this notebook, we will focus only on the `LocalCUDACluster` API (which is provided by the RAPIDS [`dask_cuda`](https://github.com/rapidsai/dask-cuda) library). I also recommend that you check out [this blog article](https://blog.dask.org/2020/07/23/current-state-of-distributed-dask-clusters) to see a high-level summary of the (many) other cluster-deployment utilities.\n",
    "\n",
    "For this example, we will assume that you want to perform preprocessing on a single machine with multiple GPUs. In this case, we can use `dask_cuda.LocalCUDACluster` to deploy a distributed cluster with each worker process being pinned to a distinct GPU.  This class also provides our workers with mechanisms for device-host memory spilling, and (optionally) enables the use of NVLink and infiniband-based inter-process communication via UCX..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-24cab4fbefd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcleint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cleint' is not defined"
     ]
    }
   ],
   "source": [
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "protocol = \"tcp\"                     # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2,3,4,5,6,7\"  # Delect devices to place workers\n",
    "device_memory_limit = \"28GB\"         # Spill device mem to host at this limit\n",
    "rmm_pool_size = \"28GB\"               # RMM pool size\n",
    "memory_limit = \"96GB\"                # Spill host mem to disk near this limit\n",
    "cluster = LocalCUDACluster(\n",
    "    protocol = protocol,\n",
    "    CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "    local_directory = dask_workdir,\n",
    "    device_memory_limit = parse_bytes(device_memory_limit),\n",
    "    memory_limit = parse_bytes(memory_limit),\n",
    ")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since allocating memory is often a performance bottleneck, it is usually a good idea to initialize a memory pool on each of our workers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RMM pool on all workers\n",
    "client.run(\n",
    "    cudf.set_allocator,\n",
    "    pool=True,\n",
    "    initial_pool_size=parse_bytes(rmm_pool_size),\n",
    "    allocator=\"default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Define our NVTabular `Workflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = [\"name\", \"id\"]\n",
    "cont_names = [\"x\", \"y\", \"timestamp\"]\n",
    "label_name = [\"label\"]\n",
    "\n",
    "workflow = nvt.Workflow(cat_names=cat_names, cont_names=cont_names, label_name=label_name, client=client)\n",
    "workflow.add_preprocess(ops.Normalize(columns=[\"x\", \"y\"]))\n",
    "workflow.add_preprocess(\n",
    "    ops.Categorify(\n",
    "        columns=[\"id\", \"name\", [\"id\", \"name\"]],\n",
    "        encode_type=\"combo\",\n",
    "        name_sep=\"+\",\n",
    "        out_path=demo_output_path,\n",
    "        cat_cache=\"device\",\n",
    "        tree_width=1,\n",
    "    )\n",
    ")\n",
    "workflow.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: (Optional) Create a Dataset Object\n",
    "\n",
    "Since we already created a Dataset above, we *could* still use it here.  However, lets create a new one since we unnecessarily shuffled that data (and deleted it anyway)...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nvt.Dataset(demo_dataset_path, part_size=\"1.2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Apply our Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.24 s, sys: 3.94 s, total: 10.2 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "workflow.apply(\n",
    "    dataset,\n",
    "    output_format=\"parquet\",\n",
    "    output_path=os.path.join(demo_output_path,\"processed\"),\n",
    "    shuffle=False,\n",
    "    out_files_per_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 GPU**:\n",
    "```\n",
    "CPU times: user 6.07 s, sys: 3.89 s, total: 9.96 s\n",
    "Wall time: 33.7 s\n",
    "```\n",
    "\n",
    "**2 GPUs**:\n",
    "```\n",
    "CPU times: user 6.19 s, sys: 3.59 s, total: 9.78 s\n",
    "Wall time: 20.1 s\n",
    "```\n",
    "\n",
    "**4 GPUs**:\n",
    "```\n",
    "CPU times: user 6.74 s, sys: 3.53 s, total: 10.3 s\n",
    "Wall time: 21.4 s\n",
    "```\n",
    "\n",
    "**8 GPUs**:\n",
    "```\n",
    "CPU times: user 6.24 s, sys: 3.94 s, total: 10.2 s\n",
    "Wall time: 13.5 s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>id+name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.242465</td>\n",
       "      <td>-0.144389</td>\n",
       "      <td>2000-10-04 21:50:24</td>\n",
       "      <td>16</td>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>3594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.810014</td>\n",
       "      <td>-0.075761</td>\n",
       "      <td>2000-10-04 21:50:25</td>\n",
       "      <td>13</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>3981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.283402</td>\n",
       "      <td>1.335227</td>\n",
       "      <td>2000-10-04 21:50:26</td>\n",
       "      <td>20</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "      <td>4508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.841540</td>\n",
       "      <td>0.419783</td>\n",
       "      <td>2000-10-05 00:33:36</td>\n",
       "      <td>13</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>3461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.290406</td>\n",
       "      <td>-0.667822</td>\n",
       "      <td>2000-10-05 03:32:48</td>\n",
       "      <td>12</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>5020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y           timestamp  name   id  label  id+name\n",
       "0  0.242465 -0.144389 2000-10-04 21:50:24    16  157      1     3594\n",
       "1  0.810014 -0.075761 2000-10-04 21:50:25    13  172      1     3981\n",
       "2  1.283402  1.335227 2000-10-04 21:50:26    20  192      1     4508\n",
       "3 -0.841540  0.419783 2000-10-05 00:33:36    13  152      1     3461\n",
       "4 -1.290406 -0.667822 2000-10-05 03:32:48    12  212      0     5020"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_cudf.read_parquet(os.path.join(demo_output_path,\"processed\")).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 519 ms, sys: 90.9 ms, total: 609 ms\n",
      "Wall time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = workflow.get_ddf()\n",
    "ddf.to_parquet(os.path.join(demo_output_path, \"dask_output\"), write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/dask-space/rzamora/demo_output/dask_output/part.1.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.25.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.19.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.10.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.24.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.20.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.11.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.12.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.21.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.5.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.17.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.27.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.3.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.4.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.8.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.26.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.6.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.28.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.13.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.16.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.9.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.14.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.22.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.0.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.2.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.7.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.15.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.18.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/part.23.parquet',\n",
       " '/raid/dask-space/rzamora/demo_output/dask_output/_metadata']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(os.path.join(demo_output_path, \"dask_output/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
