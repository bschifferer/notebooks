{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NVTabular**: Dask-based Data Processing Demo\n",
    "\n",
    "- **Author**: Rick Zamora (rzamora@nvidia.com)\n",
    "- **Date**: June 24th 2020 (last update 6/24/2020)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVTabular Background\n",
    "\n",
    "- NVTabular is a feature engineering and preprocessing library for tabular data.  A target application of NVTabular is DL-based recommender system  (RecSys) training and inference, but many other workflows may benefit.\n",
    "- Built on [RAPIDS](https://rapids.ai/)\n",
    "- Public GitHub: [NVIDIA/NVTabular](https://github.com/NVIDIA/NVTabular)\n",
    "- Part of the larger NVIDIA **Merlin** framework [See Merlin Blog for more details.](https://devblogs.nvidia.com/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/)\n",
    "\n",
    "![image.png](images/merlin_image.png)\n",
    "\n",
    "The general motivation of **NVTabular** is to leverage GPUs/CUDA to reduce the time required for the ETL/pre-processing component of the typical DL/ML model-development cycle.  It is established that engineers/scientists can benefit from GPU acceleration while training.  NVTabular hopes to prevent preprocessing from becoming the new bottleneck.\n",
    "\n",
    "![image.png](images/nvtabular_motivation.png)\n",
    "\n",
    "\n",
    "The primary goals of **NVTabular** are actually (1) data preprocessing **and** (2) data loading.  In this demo, we are focusing on **Dask-based** preprocessing with NVTabular (we will not touch on the \"data-loading\" component of NVTabular).\n",
    "\n",
    "\n",
    "## Brief Usage Overview\n",
    "\n",
    "**WARNING**: *API is likely to drift a bit over the next 1-2 months.  I will try to update this notebook as those changes happen.*\n",
    "\n",
    "Usage of the current NVTabular API looks something like the following for a simple preprocessing workflow...\n",
    "\n",
    "```python\n",
    "import nvtabular as nvt\n",
    "import nvtabular\n",
    "\n",
    "# Step 1: Initialize workflow with dataset details\n",
    "workflow = nvt.Workflow(\n",
    "    client=DASK_CLIENT,\n",
    "    cat_names=CATEGORICAL_COLUMN_NAME,\n",
    "    cont_names=CONTINUOUS_COLUMN_NAME,\n",
    "    label_name=LABEL_COLUMN_NAME,\n",
    ")\n",
    "\n",
    "# Step 2: Add feature-engineering and transform operations\n",
    "workflow.add_feature([ops.ZeroFill(), ops.LogOp()])\n",
    "workflow.add_preprocess(ops.Categorify())\n",
    "workflow.finalize()\n",
    "\n",
    "# Step 3: Create a \"dataset\" object for the workflow to \"process\"\n",
    "dataset = nvt.DaskDataset(DATSET_PATH, engine=\"parquet\", part_mem_fraction=0.2)\n",
    "\n",
    "# Step 4: Compute statistics and write shuffled/processed dataset\n",
    "workflow.apply(dataset, shuffle=\"full\", output_path=\"./proc\")\n",
    "```\n",
    "\n",
    "The overall procedure in the above code snippet is:\n",
    " \n",
    "- **Step 1**: Initialize an nvtabular `Workflow` object.  The user must distinguish the categorical and continuous columns.  A dask `client` is also needed to leverage the `dask_cudf` backend.\n",
    "- **Step 2**: Add feature-engineering operations (add new columns without changing original columns), and general pre-processing operations to the workflow.  If/when these operations require global statistics, additional \"statistics\" operators will be added to the workflow automatically.\n",
    "- **Step 3**: Create a \"dataset\" object to be operated on by the workflow.  The purpose of `DaskDataset` is to allow the workflow to effectively materialize a distributed `dask_cudf.DataFrame` object as needed.\n",
    "- **Step 4**: Execute the workflow.  May include full shuffling/transformation of data (like in the above example), but can also just collect underlying statistics if transformation will be performed \"online\".\n",
    "\n",
    "\n",
    "## Brief Algorithm Overview\n",
    "\n",
    "Under the hood, NVTabular will use the `workflow` definition to compile an internal `phases` data structure.  For the above example, `phases` will be a list of length two (two \"phases\" means two global passes over the data):\n",
    "\n",
    "```\n",
    "# phases[0]:\n",
    "\n",
    "[(<nvtabular.ops.Encoder at 0x7f3964e84390>, 'categorical', ['base'], [])]\n",
    "\n",
    "# phases[1]:\n",
    "\n",
    "[(<nvtabular.ops.ZeroFill at 0x7f3964e84a50>, 'continuous', ['base'], []),\n",
    " (<nvtabular.ops.LogOp at 0x7f3964e84250>, 'continuous', ['ZeroFill'], []),\n",
    " (<nvtabular.ops.Categorify at 0x7f3964e84ad0>,\n",
    "  'categorical',\n",
    "  ['base'],\n",
    "  [<nvtabular.ops.Encoder at 0x7f3964e84310>])]\n",
    "```\n",
    "\n",
    "In this example, phase-0 is only for the `Encoder` *statistics* operation, and phase-1 is for all other (transform) operations.  However, there is no rule that *statistics* operations and *transform* operations must happened in distinct phases.  Instead, the actual procedure for a single phase looks something like this:\n",
    "\n",
    "```python\n",
    "def exec_phase(self, phase_index, record_stats=True):\n",
    "    \"\"\"\n",
    "    Code copied from NVTabular/nvtabular/workflow.py\n",
    "\n",
    "    Gather necessary column statistics in single pass.\n",
    "    Execute one phase only, given by phase index\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    for task in self.phases[phase_index]:\n",
    "        op, cols_grp, target_cols, _ = task\n",
    "        if isinstance(op, TransformOperator):\n",
    "            stats_context = self.stats if isinstance(op, DFOperator) else None\n",
    "            logic = op.apply_op\n",
    "            transforms.append(\n",
    "                (self.columns_ctx, cols_grp, target_cols, logic, stats_context)\n",
    "            )\n",
    "        elif not isinstance(op, StatOperator):\n",
    "            raise ValueError(\"Unknown Operator Type\")\n",
    "\n",
    "    # Preform transforms as single dask task (per ddf partition)\n",
    "    if transforms:\n",
    "        self._aggregated_dask_transform(transforms)\n",
    "\n",
    "    stats = []\n",
    "    if record_stats:\n",
    "        for task in self.phases[phase_index]:\n",
    "            op, cols_grp, target_cols, _ = task\n",
    "            if isinstance(op, StatOperator):\n",
    "                stats.append(\n",
    "                    (\n",
    "                        op.dask_logic(\n",
    "                            self.get_ddf(),\n",
    "                            self.columns_ctx,\n",
    "                            cols_grp,\n",
    "                            target_cols,\n",
    "                        ), \n",
    "                        op,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Compute statistics if necessary\n",
    "    if stats:\n",
    "        for r in self.client.compute(stats):\n",
    "            computed_stats, op = r.result()\n",
    "            op.dask_fin(computed_stats)\n",
    "            self._update_stats(op)\n",
    "            op.clear()\n",
    "        del stats\n",
    "```\n",
    "\n",
    "The general summary of `exec_phase` is that *all* transform operators are bundled together and executed with a (lazy) `map_partitions` call, and then all statistics are separately bundled together and computed. This effectively means that transform ops are never computed until absolutely necessary, while statistics operations are always computed immediately. \n",
    "\n",
    "## **Real Example**: Criteo DLRM Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cudf\n",
    "import dask_cudf\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, performance_report\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "import nvtabular.ops as ops\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46761</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>768.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46761' processes=8 threads=8, memory=768.00 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup working space and spin up a LocalCUDACluster\n",
    "\n",
    "!rm -rf /raid/dask-space/rzamora/*\n",
    "!mkdir /raid/dask-space/rzamora/scratch\n",
    "worker_dir = \"/raid/dask-space/rzamora\"\n",
    "\n",
    "protocol = \"tcp\"                    # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2,3,4,5,6,7\" # Delect devices to place workers\n",
    "device_memory_limit = \"28GB\"        # Spill device mem to host at this limit\n",
    "memory_limit = \"96GB\"               # Spill host mem to disk near this limit\n",
    "\n",
    "cluster = LocalCUDACluster(\n",
    "    protocol = protocol,\n",
    "    CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "    local_directory = worker_dir,\n",
    "    device_memory_limit = parse_bytes(device_memory_limit),\n",
    "    memory_limit = parse_bytes(memory_limit),\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "client.run(\n",
    "    cudf.set_allocator,\n",
    "    pool=True,\n",
    "    initial_pool_size=parse_bytes(device_memory_limit),\n",
    "    allocator=\"default\"\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify preprocessing options/input\n",
    "\n",
    "data_path = \"/raid/dask-space/criteo/crit_pq_int\"  # Criteo Dataset (in parquet format)\n",
    "out_path = worker_dir + \"/scratch\"                 # Write output within the dask-worker space     \n",
    "nsplits = 24                                       # (Per-worker) file count for partial shuffle\n",
    "split_out = 8                                      # Number of hash splits for groupby reductions\n",
    "cat_cache = \"host\"                                 # Where to cache categories between transform ops\n",
    "on_host = True                                     # Move data to host between Groupby tasks\n",
    "part_mem_fraction = 0.2                            # Size of each dask partition (fraction of device)\n",
    "\n",
    "cont_names = ['I' + str(x) for x in range(1,14)]\n",
    "cat_names =  ['C' + str(x) for x in range(1,27)]\n",
    "label_name = [\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a demo, lets use a subset of columns\n",
    "# (Full dataset should work fine, but takes a couple minutes longer)\n",
    "\n",
    "cont_names = cont_names[:4]\n",
    "cat_names = cat_names[:4]\n",
    "\n",
    "# Save list of all columns for convenience\n",
    "columns=cat_names + cont_names + label_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a `Workflow` as in the simple example above.  However, we are now including arguments to the `Categorify` operation.  The meaning of these options will be discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NVTabular Workflow\n",
    "\n",
    "workflow = nvt.Workflow(\n",
    "    client=client,\n",
    "    cat_names=cat_names,\n",
    "    cont_names=cont_names,\n",
    "    label_name=label_name,\n",
    ")\n",
    "\n",
    "workflow.add_feature([ops.ZeroFill(), ops.LogOp()])\n",
    "\n",
    "workflow.add_preprocess(\n",
    "    ops.Categorify(\n",
    "        out_path=out_path,\n",
    "        split_out=split_out,\n",
    "        cat_cache=cat_cache,\n",
    "        on_host=on_host,\n",
    "    )\n",
    ")\n",
    "workflow.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a `dataset` object for the `workflow` to operate on\n",
    "\n",
    "dataset = nvt.DaskDataset(data_path, engine=\"parquet\", part_mem_fraction=part_mem_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth highlighting that `DaskDataset` *should* eventually become a thin wrapper around `dask_cudf.read_parquet/csv/orc`.  However, for performance reasons, we have temporarily implemented a replacement for `dask_cudf.read_parquet`.  Once reason is that we want to allow cudf to read multiple row-groups at once (which `dask_cudf`/`dask.dataframe` doesn't allow **yet**).  The other reason is related to the \"planning\" stage of the read...\n",
    "\n",
    "Within NVTabular, it is important that the client be able to **quickly** construct a new `dask_cudf.DataFrame`, with a specified partition size, on demand.  That is, the following behavior is necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 ms, sys: 27.7 ms, total: 64.9 ms\n",
      "Wall time: 59 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=156</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: parquet-to-ddf, 156 tasks</div>"
      ],
      "text/plain": [
       "<dask_cudf.DataFrame | 156 tasks | 156 npartitions>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dataset.to_ddf(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently a challenge for `dask_cudf.read_parquet`.  Even if we **don't** use the `chunksize` argument, partitioning by row-group is already painfully slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.6 s, sys: 2.55 s, total: 54.1 s\n",
      "Wall time: 47.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=6337</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 6337 tasks</div>"
      ],
      "text/plain": [
       "<dask_cudf.DataFrame | 6337 tasks | 6337 npartitions>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dask_cudf.read_parquet(data_path, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that the reason(s) for the poor performance of `dask_cudf.read_parquet` **can** be resolved, and I am currently working on them :)\n",
    "\n",
    "**Finally, executing the workflow:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.67 s, sys: 703 ms, total: 8.37 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "workflow.apply(dataset, shuffle=\"full\", nsplits=nsplits, output_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.791759</td>\n",
       "      <td>5.424950</td>\n",
       "      <td>3.044523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106215871</td>\n",
       "      <td>37607</td>\n",
       "      <td>1</td>\n",
       "      <td>5877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.079442</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>192250877</td>\n",
       "      <td>28189</td>\n",
       "      <td>8934</td>\n",
       "      <td>3304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.791759</td>\n",
       "      <td>5.420535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>66120273</td>\n",
       "      <td>25</td>\n",
       "      <td>15210</td>\n",
       "      <td>3304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.726848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40551666</td>\n",
       "      <td>11989</td>\n",
       "      <td>1597</td>\n",
       "      <td>6637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.477337</td>\n",
       "      <td>6.390241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.586172</td>\n",
       "      <td>31075927</td>\n",
       "      <td>24231</td>\n",
       "      <td>6273</td>\n",
       "      <td>6713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4         C1     C2     C3    C4  \\\n",
       "0  1.791759  5.424950  3.044523  0.000000  106215871  37607      1  5877   \n",
       "1  2.079442  5.010635  0.000000  0.000000  192250877  28189   8934  3304   \n",
       "2  1.791759  5.420535  0.000000  2.302585   66120273     25  15210  3304   \n",
       "3  0.000000  5.726848  0.000000  0.000000   40551666  11989   1597  6637   \n",
       "4  4.477337  6.390241  0.000000  6.586172   31075927  24231   6273  6713   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the first row-group of the output dataset looks \"processed\"\n",
    "\n",
    "cudf.io.read_parquet(\n",
    "    glob.glob(worker_dir + \"/scratch/processed/*\")[0],\n",
    "    row_group=0,\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned for `dask`/`dask_cudf`\n",
    "\n",
    "- `read_parquet` performance needs work (performance improvements are in progress here)\n",
    "- `cudf`'s chunked parquet writer and `cudf`'s ability to write-to/read-from a `BytesIO` object allows per-worker data shuffling to avoid communication and device memory pressure.\n",
    "- `groupby`-based statistics required the implementation of a custom dask-based groupby algorithm.\n",
    "- For categorical encoding, the fastest caching strategy for the categorical labels required writing-to and reading-from a `BytesIO` objects in host memory.\n",
    "\n",
    "### `read_parquet` Performance\n",
    "\n",
    "As mentioned above, this issue will be resolved in the near future. `dask.dataframe` currently combines row-group statistics gathering with row-group splitting in an unnecessary way.  There is also some very slow code in the statistics processing.\n",
    "\n",
    "\n",
    "### Shuffling with \"Chunked\" Parquet\n",
    "\n",
    "Although a *true* shuffle will be implemented/included in the near future,  the need for a \"stable\" shuffling approach (with limited global device memory) has led to \"full\" and \"partial\" **per-worker** shuffling for now.  For example, the \"full\" per-worker shuffle will perform a full shuffle of all data transformed by **that** worker (no mixing between workers).  This approach may be sufficient for may applications, but we will certainly provide a **true** shuffle for users who need it.\n",
    "\n",
    "![image.png](images/per_worker_shuffle.png)\n",
    "\n",
    "\n",
    "### Custom `groupby` Implementation for Categorical Encoding\n",
    "\n",
    "Since dataset statistics are (or *should* be) persisted to disk in NVTabular.  We want a Dask-based `groupby` operation to simply return the path of the parquet file where groupby statistics have been written for a specific categorical column (or combination of columns). This *can* be accomplished with the existing `dask_cudf`/`dask.dataframe` API (by using specific `split_every`/`split_out` parameters and adding a delayed `cudf.to_parquet` for each result), but our \"custom\" implementation has provided better stability and performance.  The reasons for this:\n",
    "\n",
    "- We are using a simple hash-based tree reduction for each categorical column. The user can specify a different number of hash partitions (width of the tree) for each column if necessary.\n",
    "- The groupby operation for all categorical columns will share the same \"top-level\" task for each ddf partition.\n",
    "- The final parquet write is included within the leaf nodes of the simple tree reduction (reducing the number of overall tasks).\n",
    "- For systems with limited device memory, the `on_host=True` option can be used to dramatically reduce memory pressure.  This will convert cudf DataFrames to pandas **between** tree nodes, and leave the data in pandas format for initial `concat` operations.\n",
    "\n",
    "\n",
    "For example, some of the relevant code is copied here:\n",
    "\n",
    "\n",
    "```python\n",
    "def _top_level_groupby(gdf, cat_cols, split_out, cont_cols, sum_sq, on_host):\n",
    "    # Top-level operation for category-based groupby aggregations\n",
    "    output = {}\n",
    "    k = 0\n",
    "    for i, cat_col in enumerate(cat_cols):\n",
    "\n",
    "        # Compile aggregation dictionary and add \"squared-sum\"\n",
    "        # column(s) (necessary when `cont_cols` is non-empty)\n",
    "        df_gb = gdf[[cat_col] + cont_cols].copy(deep=False)\n",
    "        agg_dict = {}\n",
    "        agg_dict[cat_col] = [\"count\"]\n",
    "        for col in cont_cols:\n",
    "            agg_dict[col] = [\"sum\"]\n",
    "            if sum_sq:\n",
    "                name = _make_name(col, \"pow2\")\n",
    "                df_gb[name] = df_gb[col].pow(2)\n",
    "                agg_dict[name] = [\"sum\"]\n",
    "\n",
    "        # Perform groupby and flatten column index\n",
    "        # (flattening provides better cudf support)\n",
    "        gb = df_gb.groupby(cat_col, dropna=False).agg(agg_dict)\n",
    "        gb.columns = [\n",
    "            _make_name(*name)\n",
    "            if name[0] == cat_col\n",
    "            else _make_name(*((cat_col,) + name))\n",
    "            for name in gb.columns.to_flat_index()\n",
    "        ]\n",
    "        gb.reset_index(inplace=True, drop=False)\n",
    "        del df_gb\n",
    "\n",
    "        # Split the result by the hash value of the categorical column\n",
    "        for j, split in enumerate(\n",
    "            gb.partition_by_hash(\n",
    "                [cat_col], split_out[cat_col], keep_index=False\n",
    "            )\n",
    "        ):\n",
    "            if on_host:\n",
    "                output[k] = split.to_pandas()\n",
    "            else:\n",
    "                output[k] = split\n",
    "            k += 1\n",
    "        del gb\n",
    "    return output\n",
    "\n",
    "\n",
    "def _mid_level_groupby(dfs, col, cont_cols, agg_list, freq_limit, on_host):\n",
    "    ignore_index = True\n",
    "    if on_host:\n",
    "        gb = cudf.from_pandas(\n",
    "            _concat(dfs, ignore_index)\n",
    "        ).groupby(col, dropna=False).sum()\n",
    "    else:\n",
    "        gb = _concat(dfs, ignore_index).groupby(col, dropna=False).sum()\n",
    "    gb.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    name_count = _make_name(col, \"count\")\n",
    "    if freq_limit:\n",
    "        gb = gb[gb[name_count] >= freq_limit]\n",
    "\n",
    "    required = [col]\n",
    "    if \"count\" in agg_list:\n",
    "        required.append(name_count)\n",
    "        \n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "### Parquet + BytesIO Proved Most efficient for Category Caching \n",
    "\n",
    "When applying the `Categorify` operation, a dask worker will need to read in the categorical labels for each column within each transformation task.  Since the worker will be reading the same data many times, there is an obvious opportunity to cache data in host and device memory.\n",
    "\n",
    "Although we tried a variety of techniques to implement host-memory caching, the clear performance winner was `read_parquet` + `BytesIO`.  For example, the relevant code is copied below...\n",
    "\n",
    "\n",
    "```python\n",
    "def get_categories(self, col, path, cache=\"disk\"):\n",
    "    table = self.cat_cache.get(col, None)\n",
    "    if table and not isinstance(table, cudf.DataFrame):\n",
    "        df = cudf.io.read_parquet(table, index=False, columns=[col])\n",
    "        df.index.name = \"labels\"\n",
    "        df.reset_index(drop=False, inplace=True)\n",
    "        return df\n",
    "\n",
    "    if table is None:\n",
    "        if cache in (\"device\", \"disk\"):\n",
    "            table = cudf.io.read_parquet(path, index=False, columns=[col])\n",
    "        elif cache == \"host\":\n",
    "            with open(path, \"rb\") as f:\n",
    "                self.cat_cache[col] = BytesIO(f.read())\n",
    "            table = cudf.io.read_parquet(self.cat_cache[col], index=False, columns=[col])\n",
    "        table.index.name = \"labels\"\n",
    "        table.reset_index(drop=False, inplace=True)\n",
    "        if cache == \"device\":\n",
    "            self.cat_cache[col] = table.copy(deep=False)\n",
    "    return table\n",
    "```\n",
    "\n",
    "\n",
    "# Thanks!\n",
    "\n",
    "Please feel free to contact me directly with comments/suggestions/questions.  Also, NVTabular (especially the dask-based backend) is still a work in progress, so this notebook is likely to become obsolete fairly quickly (sorry in advance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
